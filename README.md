# nlp-toxic-comments
Mitigating Bias on Different Toxic Comment Classification Models | Final Project DS-GA 1011 Natural Language Processing

In this project, we analyze the effects of bias in datasets and experiment with different language models to compare which is least vulnerable to inconsistencies in data. Specifically, we build models to analyze the toxicity level of a comment and evaluate its performance. Following this, we use different metrics to detect learned bias in the models and compare their results. This project suggests methods that can help improve both the validity of data as well as the performance of the model to estimate bias in text. 
